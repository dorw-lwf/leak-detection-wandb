{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -U sagemaker s3fs h5py wandb[launch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea7cea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72500a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB login\n",
    "!wandb login API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for SageMaker Pipeline\n",
    "prefix = \"pipeline-model-example\"\n",
    "pipeline_name = \"serial-inference-pipeline\"\n",
    "raw_s3 = \"s3://das-samples-uploader/DS_Ramsbrook_DAS_data/2024-04-18/100_lpm_60s_1\"\n",
    "preprocessing_machine = \"ml.m5.12xlarge\"\n",
    "training_machine = \"ml.m5.12xlarge\"\n",
    "validation_machine = \"ml.m5.12xlarge\"\n",
    "\n",
    "# Hyperparameters\n",
    "training_epochs = \"10\"\n",
    "accuracy_threshold = 0.75\n",
    "tensorflow_version = \"2.4.1\"\n",
    "python_version = \"py37\"\n",
    "DAS_data_channels = 376"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6004c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up SageMaker and pipeline session\n",
    "sess = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "role = get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(region, bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54499f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for SageMaker Workflow\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString, ParameterFloat\n",
    "\n",
    "input_data = ParameterString(name=\"InputData\", default_value=raw_s3)\n",
    "model_approval_status = ParameterString(name=\"ModelApprovalStatus\", default_value=\"Approved\")\n",
    "processing_instance_type = ParameterString(name=\"ProcessingInstanceType\", default_value=preprocessing_machine)\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "training_instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=training_machine)\n",
    "training_epochs_param = ParameterString(name=\"TrainingEpochs\", default_value=training_epochs)\n",
    "accuracy_mse_threshold = ParameterFloat(name=\"AccuracyMseThreshold\", default_value=accuracy_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180315f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/preprocess.py\n",
    "# Preprocess Data Script\n",
    "import os\n",
    "from my_package.read_DAS_hdf5 import (\n",
    "    load_multi_DAS_file, list_hdf5_files_in_dir, generate_training_set_spectrogram\n",
    ")\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_location = f\"{base_dir}/input/\"\n",
    "    file_names = list_hdf5_files_in_dir(data_location)\n",
    "    DAS_filtered_data = load_multi_DAS_file(file_names, channels=376)\n",
    "    \n",
    "    # Generate datasets\n",
    "    training_data_leak = generate_training_set_spectrogram(\n",
    "        DAS_filtered_data, 188, 199, [300000, 2182000], 20000\n",
    "    )\n",
    "    training_data_noleak = generate_training_set_spectrogram(\n",
    "        DAS_filtered_data, 148, 270, [0, 200000], 20000\n",
    "    )\n",
    "\n",
    "    # Combine and save datasets\n",
    "    training_data = np.concatenate([training_data_leak, training_data_noleak])\n",
    "    training_label = np.concatenate([np.ones(len(training_data_leak)), np.zeros(len(training_data_noleak))])\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        training_data, training_label, test_size=0.33, random_state=7\n",
    "    )\n",
    "    np.save(f\"{base_dir}/train/X_train.npy\", X_train)\n",
    "    np.save(f\"{base_dir}/train/y_train.npy\", y_train)\n",
    "    np.save(f\"{base_dir}/test/X_test.npy\", X_test)\n",
    "    np.save(f\"{base_dir}/test/y_test.npy\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbe27a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "# Processing Step Definition\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=preprocessing_machine,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "        ProcessingInput(source=\"code/\", destination=\"/opt/ml/processing/depend/code\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocess.py\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e9a454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "step_process = ProcessingStep(name=\"PreprocessData\", step_args=processor_args)\n",
    "\n",
    "print(bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b3fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/train.py\n",
    "# Model Training Script\n",
    "import argparse\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input, Sequential\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--train\", type=str, default=os.environ.get(\"SM_CHANNEL_TRAIN\"))\n",
    "    parser.add_argument(\"--test\", type=str, default=os.environ.get(\"SM_CHANNEL_TEST\"))\n",
    "    parser.add_argument(\"--sm-model-dir\", type=str, default=os.environ.get(\"SM_MODEL_DIR\"))\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_data(data_dir, type=\"train\"):\n",
    "    x_data = np.load(f\"{data_dir}/X_{type}.npy\")\n",
    "    y_data = np.load(f\"{data_dir}/y_{type}.npy\")\n",
    "    return x_data, y_data\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = Sequential([\n",
    "        Input(shape=(5, 29, 89, 1)),\n",
    "        layers.Conv3D(128, (3, 3, 3), activation='relu', bias_initializer=Constant(0.01)),\n",
    "        layers.Conv3D(128, (3, 3, 3), activation='relu', bias_initializer=Constant(0.01)),\n",
    "        layers.MaxPooling3D((2, 2, 2)),\n",
    "        layers.Conv3D(64, (3, 3, 3), activation='relu', padding='same'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ])\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    x_train, y_train = get_data(args.train)\n",
    "    x_test, y_test = get_data(args.test)\n",
    "\n",
    "    model = get_model(num_classes=2)\n",
    "    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs, validation_data=(x_test, y_test))\n",
    "    model.save(f\"{args.sm_model_dir}/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703eba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "hyperparameters = {\"epochs\": training_epochs, \"learning_rate\": 0.1, \"batch_size\": 64}\n",
    "model_path = f\"s3://{bucket}/{prefix}/model/\"\n",
    "\n",
    "tf2_estimator = TensorFlow(\n",
    "    source_dir=\"code\",\n",
    "    entry_point=\"train.py\",\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    framework_version=tensorflow_version,\n",
    "    role=role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=model_path,\n",
    "    py_version=python_version,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "train_args = tf2_estimator.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri),\n",
    "        \"test\": TrainingInput(s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri),\n",
    "    }\n",
    ")\n",
    "\n",
    "step_train_model = TrainingStep(name=\"TrainTensorflowModel\", step_args=train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data, model_approval_status, training_epochs_param, accuracy_mse_threshold],\n",
    "    steps=[step_process, step_train_model],\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "\n",
    "execution.describe()\n",
    "execution.wait()\n",
    "execution.list_steps()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
